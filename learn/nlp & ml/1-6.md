用python进行自然语言处理，github版：https://github.com/nltk/nltk_book/tree/master/book



获取文本
txt
url
html
blog

编码：unicode

## 正则表达式
搜索号码盘上的组合：
[w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]

### 注意()的用法
提取词干：
>>> def stem(word):
... for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:
... if word.endswith(suffix):
... return word[:-len(suffix)]
... return word

使用正则表达式：
>>> re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')
['processing']

>>> re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')
[('process', 'ing')]

### 注意<>的用法，尖括号之间的所有空白都被忽略（这只对NLTK 中的findall()方法处理文本有效）
>>> moby = nltk.Text("a monied man")
>>> moby.findall(r"<a> (<.*>) <man>")
monied

### 发现上位词
当特定语言现象与特定词语（结构）相关时，x and the ys，很可能y是x的上位词
>>> from nltk.corpus import brown
>>> hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))
>>> hobbies_learned.findall(r"<\w*> <and> <other> <\w*s>")
speed and other activities; water and other liquids; tomb and other
landmarks; Statues and other monuments; pearls and other jewels;

## 词干提取
NLTK 中包括了一些现成的词干提取器，如果你需要一个词干提取器，你应该优先使用
它们中的一个，而不是使用正则表达式制作自己的词干提取器，因为NLTK 中的词干提取
器能处理的不规则的情况很广泛
>>> tokens = nltk.word_tokenize(raw)
>>> porter = nltk.PorterStemmer()
>>> lancaster = nltk.LancasterStemmer()
>>> [porter.stem(t) for t in tokens]


### 词形归并
与词干提取不同的时，所有词都会归并到字典中出现过的词。请注意，它并没有处理“lying”，但它将“wom
en”转换为“woman”。

>>> wnl = nltk.WordNetLemmatizer()
>>> [wnl.lemmatize(t) for t in tokens]

### 分词
正则表达式分词器
>>> pattern = r'''(?x) # set flag to allow verbose regexps
... ([A-Z]\.)+ # abbreviations, e.g. U.S.A.
... | \w+(-\w+)* # words with optional internal hyphens
... | \$?\d+(\.\d+)?%? # currency and percentages, e.g. $12.40, 82%
... | \.\.\. # ellipsis
... | [][.,;"'?():-_`] # these are separate tokens
... '''
>>> nltk.regexp_tokenize(text, pattern)
['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']
我们可以使用set(tokens).difference(wordlist)，通过比较分词结果与
一个词表，然后报告任何没有在词表出现的标识符，来评估一个分词器。


### 标注
* 默认标注器：每个此的标注都设置为相同
```
default_tagger = nltk.DefaultTagger('NN')

```
* 正则表达式标注器：利用正则表达式进行简单的匹配，如ed结尾的为动词的过去分词
```
regexp_tagger = nltk.RegexpTagger(patterns)

```
* 查找标注器：找到前100个最频繁的词，标注它们。仅对文本中的这100个词标记。其实已经能完成40%的内容
```
>>> baseline_tagger = nltk.UnigramTagger(model=likely_tags,
... backoff=nltk.DefaultTagger('NN'))

```
* N-gram，一元标注器：类似于查找标注，每个词只有一种标注。输入为一个已经标注完成的句子语料
```
>>> from nltk.corpus import brown
>>> brown_tagged_sents = brown.tagged_sents(categories='news')
>>> unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)

```
* 一般N-gram：**考虑上下文，基于统计，但不跨越句子**，一个n-gram标注器是一个unigram标注器的一般化，上下文是当前词和前面n-1个标示符的词性标记
```
>>> unigram_tagger = nltk.UnigramTagger(train_sents)   1-gram
>>> bigram_tagger = nltk.BigramTagger(train_sents)     2-gram
>>> bigram_tagger = nltk.TrigramTagger(train_sents)    3-gram

```
* 组合标注器：通常n-gram的n越大，如果训练数据不足，则很可能导致结果越差，使用组合标记器，有一定的缓解作用
```
>>> t0 = nltk.DefaultTagger('NN')
>>> t1 = nltk.UnigramTagger(train_sents, backoff=t0)
>>> t2 = nltk.BigramTagger(train_sents, backoff=t1)
>>> t2.evaluate(test_sents)
0.84491179108940495

```

### 分类
步骤：
* 关键在于特征的选取
* 然后选取相关的分类器（如决策树、naivebayes等）进行分类
* 最后评估精度，根据分类出错的部分优化特征
* 得到训练好的分类器，dump到文件

一些例子：
* 选取单词结尾字母为特征，判断一个name是male or female
* 根据邮件是否包含某个词，判断邮件是否垃圾有键
* 判断评论的positive or negtive
* 根据强两个词的词性、当前词的结构（如结尾字母），判断当前词的词性。这种分类器实际就是一个n-gram标注器

```python
def gender_features2(name):
features = {}
features["firstletter"] = name[0].lower()
features["lastletter"] = name[–1].lower()
for letter in 'abcdefghijklmnopqrstuvwxyz':
features["count(%s)" % letter] = name.lower().count(letter)
features["has(%s)" % letter] = (letter in name.lower())
return features
>>> gender_features2('John')
{'count(j)': 1, 'has(d)': False, 'count(b)': 0, ...}

```
注意到，上述的例子特征都很不数值化（如是否包含某个单词，结尾字母是什么等），因此选择的分类器也主要是NaiveBayes和决策树这两种，靠概率和if-else即可实现的，而不是涉及到数值计算的逻辑回归等分类器。而要使用更复杂的分类器，需要将特征数值化

### 识别文字蕴含 RTE
T: Parviz Davudi was representing Iran at a meeting of the Shanghai Co-operation
Organisation (SCO), the fledgling association that binds Russia, China and four
former Soviet republics of central Asia together to fight terrorism.
H: China is a member of SCO.
Challenge 3, Pair 81 (False)
T: According to NC Articles of Organization, the members of LLC company are
H. Nelson Beavers, III, H. Chester Beavers and Jennie Beavers Stewart.
H: Jennie Beavers Stewart is a share-holder of Carolina Analytical Laboratory.

### 评估
训练集（开发训练集和开发测试集），测试集（评估准确率）
准确率：正确标注的比例
真阳性TP（正确识别为相关的），真阴性TN，假阳性FP，假阴性FN
精确度：我们发现的项目中有多少真正相关，TP/(TP+FP)
召回率：相关的项目中我们发现了多少，TP/(TP+FN)
F- 度量值：精确度与召回率的调和平均数
混淆矩阵：
留一法交叉验证：假设有N个样本，将每一个样本作为测试样本，其它N-1个样本作为训练样本。这样得到N个分类器，N个测试结果。用这N个结果的平均值来衡量模型的性能。
普通交叉验证：我理解的是K倍交叉验证（k-fold cross validation）：将所有样本分成K份，一般每份样本的数量相等或相差不多。取一份作为测试样本，剩余K-1份作为训练样本。这个过程重复K次，最后的平均测试结果可以衡量模型的性能。


### apply
知识图谱：我们可以把它理解成一张由知识点相互连接而成的语义网络http://www.jianshu.com/p/76263e5365a9
语言模型,简单的定义是:给定一句话(sentence),这句话出现的可能性有多大;或给定连续的N个词 (words),那么第N+1个词 (word) 是什么,概率是多少。

## 
seq2seq：www.tuicool.com/articles/7ZBzAzf
* 翻译，英语到法语
* 摘要，图片到文字描述
* 聊天机器人，问题到回答
* 等等

基于CNN的seq2seq，在翻译上已经比基于RNN的seq2seq更好？https://www.zhihu.com/question/59697263/answer/168866935

ipython形式的nltk教程：https://github.com/FCTweedie/NLTK

## 报错

### No module named _sqlite3
* Install sqlite-devel (or libsqlite3-dev on some Debian-based systems)
* re-configured and re-compiled Python with ./configure --enable-loadable-sqlite-extensions && make && sudo make install

### Resource 'tokenizers/punkt/PY3/english.pickle' not found.
>>> nltk.download("punkt")